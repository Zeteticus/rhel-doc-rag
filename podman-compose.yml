version: '3'

services:
  # Vector database - uses Qdrant
  vector-db:
    image: docker.io/qdrant/qdrant:v1.4.1
    volumes:
      - ./vector_data:/qdrant/storage
    ports:
      - "6333:6333"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--spider", "http://localhost:6333/health"]
      interval: 5s
      timeout: 5s
      retries: 3

  # Data processor - waits for vector-db to be ready
  data-processor:
    build:
      context: ./data-processor
      dockerfile: Containerfile
    volumes:
      - ./downloaded_docs:/data/documents
    depends_on:
      vector-db:
        condition: service_healthy
    environment:
      - VECTOR_DB_HOST=vector-db
      - VECTOR_DB_PORT=6333
    # Only run once to load data, then exit
    restart: "no"

  # LLM service - for generating responses
  llm-service:
    build:
      context: ./llm-service
      dockerfile: Containerfile
    ports:
      - "8080:8080"
    environment:
      - MCP_SERVER_URL=http://localhost:8090
    # If using local models
    volumes:
      - ./models:/app/models

  # API service - waits for both vector-db and llm-service
  api-service:
    build:
      context: ./api-service
      dockerfile: Containerfile
    ports:
      - "5000:5000"
    depends_on:
      vector-db:
        condition: service_healthy
      llm-service:
        condition: service_started
    environment:
      - VECTOR_DB_HOST=vector-db
      - VECTOR_DB_PORT=6333
      - LLM_SERVICE_HOST=llm-service
      - LLM_SERVICE_PORT=8080

  # Web interface - waits for API service
  web-interface:
    build:
      context: ./web-interface
      dockerfile: Containerfile
    ports:
      - "80:80"
    depends_on:
      - api-service
